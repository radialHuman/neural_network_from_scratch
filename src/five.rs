// https://www.youtube.com/watch?v=gmjzbpSVY1A&t=5s

/*
Activation function : relu, step and sigmoid (for hidden layers)
Data read in manually, generated using python code from video, create_data function incomplete
*/
use math::round;
use ndarray::Array;
use ndarray_rand::rand_distr::{Distribution, Normal};
use rand::*;
use std::fs;
use std::io;

// from four::four_oopsstruct
struct LayerDetails {
    n_inputs: usize,
    n_neurons: i32,
}
impl LayerDetails {
    pub fn create_weights(&self) -> Vec<Vec<f64>> {
        let mut rng = rand::thread_rng();
        let mut weight: Vec<Vec<f64>> = vec![];
        // this gives transposed weights
        for _ in 0..self.n_inputs {
            weight.push(
                (0..self.n_neurons)
                    .map(|_| round::ceil(rng.gen_range(-1., 1.), 3))
                    .collect(),
            );
        }
        weight
    }
    pub fn create_bias(&self) -> Vec<f64> {
        let bias = vec![0.; self.n_neurons as usize];
        bias
    }
    pub fn output_of_layer(
        &self,
        input: &Vec<Vec<f64>>,
        weights: &Vec<Vec<f64>>,
        bias: &Vec<f64>,
    ) -> Vec<Vec<f64>> {
        let mat_mul = transpose(&matrix_product(&input, &weights));
        // println!("input * weights = {:?}", mat_mul);
        let mut output: Vec<Vec<f64>> = vec![];
        for i in mat_mul {
            // println!("i*w {:?}, bias {:?}", &i, &bias);
            output.push(vector_addition(&i, &bias));
        }
        println!("Before activation it was {:?}", &output[0]);
        println!("After activation it was {:?}", activation_relu(&output[0]));
        transpose(&output)
    }
}

pub fn five() {
    /*ACTIVATION FUNCTION
     * Aim: If there are no AF, its y = x (linear function), this will not helpw ith non-linear relations
     */

    /* STEP FUNCTIONS
     * If the input is more than 0 then output is 1 else 0
     * Input here is the input*weight+bias
     * Output is either 1 or 0
     * Not useful in output layer
     * Not really useful in nn
     */

    /* SIGMOID FUNCTIONS
     * y = 1/(1+e^(-x))
     * Gives granular output
     * Useful for optimizing by reducing loss
     * Cant avoid vanishing gradient problem
     */

    /* RECTIFIED LINEAR UNIT
     * If its less than 0 its 0 else what ever x is
     * Its fast and no calculation involved
     * It works because, clipping it at 0 makes it good for non-linear fucntions
     * Weight moves line away from x axis
     * Bias offsets line along -x axis if positive
     * Decides when the neuron activates and when it does not
     * The combination of weights and baisa using this function can fit non-linear functions
     * More the neurons the better the fit
     */

    let input1 = vec![0., 2., -1., 3.3, -2.7, 1.1, 2.2, -100.];
    let output = activation_relu(&input1);
    println!("The output from relu for {:?} is {:?}", &input1, &output);

    // using the create_data function
    let (X, y) = create_data(100, 3, 2);

    // First layer
    // OOPS
    let l1 = LayerDetails {
        n_inputs: X[0].len(), // number of features/columns in X
        n_neurons: 5,         // can be anything depending the situation
    };
    println!("The input of layer 1 are = {}x{}", X.len(), X[0].len());
    print_a_matrix("The matrix is:", &X);
    let generated_weights1 = l1.create_weights();
    println!(
        "The weights of layer 1 are = {}x{}",
        generated_weights1.len(),
        generated_weights1[0].len()
    );
    print_a_matrix("Generated weights are:", &generated_weights1);
    // print_a_matrix(&transpose(&generated_weights1));
    let generated_bias1 = l1.create_bias();
    println!(
        "The bias of layer 1 is  = 1x{}\n{:?}\n",
        generated_bias1.len(),
        &generated_bias1
    );

    let generated_output1 =
        transpose(&l1.output_of_layer(&X, &generated_weights1, &generated_bias1));
    println!(
        "The output of layer 1 are = {}x{}",
        generated_output1.len(),
        generated_output1[0].len()
    );
    print_a_matrix("Output before activation is:", &generated_output1);

    // output through activation
    let mut activated_output_1 = vec![];
    for i in generated_output1 {
        activated_output_1.push(activation_relu(&i));
    }
    print_a_matrix("Input to second layer is:", &activated_output_1);

    // read in X
    let X_string = read_file("./src/spiral_data_X.txt").unwrap();
    let X_inter = &X_string
        .replace("[", "")
        .replace("]", "")
        .replace("\n", "")
        .replace(" ", "");
    let X_vector: Vec<_> = X_inter.split(",").collect();
    // println!("Input is {:?}", X_vector);
    let X_parsed: Vec<f64> = X_vector.iter().map(|x| x.parse().unwrap()).collect();
    let features = shape_changer(&X_parsed, 2, X_parsed.len() / 2);
    // println!("Input is {:?}", features);

    // read in y
    let y_string = read_file("./src/spiral_data_y.txt").unwrap();
    let y_inter = &y_string.replace("[", "").replace("]", "").replace(" ", "");
    let y_vector: Vec<_> = y_inter.split(",").collect();
    // println!("Input is {:?}", X_vector);
    let target: Vec<f64> = y_vector.iter().map(|x| x.parse().unwrap()).collect();
    // println!("Input is {:?}", target);

    // First layer
    let X = features; // input in batches
                      // OOPS
    let l1 = LayerDetails {
        n_inputs: X[0].len(), // number of features/columns in X
        n_neurons: 5,         // can be anything depending the situation
    };
    println!("The input of layer 1 are = {}x{}", X.len(), X[0].len());
    print_a_matrix("The matrix is:", &X);
    let generated_weights1 = l1.create_weights();
    println!(
        "The weights of layer 1 are = {}x{}",
        generated_weights1.len(),
        generated_weights1[0].len()
    );
    print_a_matrix("Generated weights are:", &generated_weights1);
    // print_a_matrix(&transpose(&generated_weights1));
    let generated_bias1 = l1.create_bias();
    println!(
        "The bias of layer 1 is  = 1x{}\n{:?}\n",
        generated_bias1.len(),
        &generated_bias1
    );

    let generated_output1 =
        transpose(&l1.output_of_layer(&X, &generated_weights1, &generated_bias1));
    println!(
        "The output of layer 1 are = {}x{}",
        generated_output1.len(),
        generated_output1[0].len()
    );
    print_a_matrix("Output before activation is:", &generated_output1);

    // output through activation
    let mut activated_output_1 = vec![];
    for i in generated_output1 {
        activated_output_1.push(activation_relu(&i));
    }
    print_a_matrix("Input to second layer is:", &activated_output_1);
}

pub fn read_file(path: &str) -> Result<String, io::Error> {
    fs::read_to_string(path)
}

pub fn transpose(matrix: &Vec<Vec<f64>>) -> Vec<Vec<f64>> {
    let mut output = vec![];
    for j in 0..matrix[0].len() {
        for i in 0..matrix.len() {
            output.push(matrix[i][j]);
        }
    }
    let x = matrix[0].len();
    shape_changer(&output, matrix.len(), x)
}

pub fn shape_changer<T>(list: &Vec<T>, columns: usize, rows: usize) -> Vec<Vec<T>>
where
    T: std::clone::Clone,
{
    /*Changes a list to desired shape matrix*/
    // println!("{},{}", &columns, &rows);
    let mut l = list.clone();
    let mut output = vec![vec![]; rows];
    for i in 0..rows {
        output[i] = l[..columns].iter().cloned().collect();
        // remove the ones pushed to putput
        l = l[columns..].iter().cloned().collect();
    }
    output
}

pub fn print_a_matrix(string: &str, matrix: &Vec<Vec<f64>>) {
    println!("{}", string);
    for i in matrix.iter() {
        println!("{:?}", i);
    }
    println!("");
    println!("");
}

pub fn activation_relu(input: &Vec<f64>) -> Vec<f64> {
    input
        .iter()
        .map(|x| if *x > 0. { *x } else { 0. })
        .collect()
}

pub fn vector_addition(a: &Vec<f64>, b: &Vec<f64>) -> Vec<f64> {
    let mut output = vec![];
    for i in 0..a.len() {
        output.push(round::ceil(a[i], 3) + round::ceil(b[i], 3));
    }
    output
}

pub fn matrix_product(input: &Vec<Vec<f64>>, weights: &Vec<Vec<f64>>) -> Vec<Vec<f64>> {
    // println!(
    //     "Multiplication of {}x{} and {}x{}",
    //     input.len(),
    //     input[0].len(),
    //     weights.len(),
    //     weights[0].len()
    // );
    // println!("Weights transposed to",);
    let weights_t = transpose(&weights);
    // print_a_matrix(&weights_t);
    let mut output: Vec<f64> = vec![];
    for i in input.iter() {
        for j in weights_t.iter() {
            // println!("{:?}x{:?},", i, j);
            output.push(dot_product(&i, &j));
        }
    }
    // println!("{:?}", output);
    shape_changer(&output, input.len(), weights_t.len())
}

pub fn dot_product(a: &Vec<f64>, b: &Vec<f64>) -> f64 {
    a.iter().zip(b.iter()).map(|(x, y)| *x * *y).sum()
}

pub fn create_data(
    data_points: i32,
    classes: i32,
    dimension: usize,
) -> (std::vec::Vec<std::vec::Vec<f64>>, std::vec::Vec<i32>) {
    // -> (Vec<f64>, Vec<u8>) {
    // https://github.com/cs231n/cs231n.github.io/blob/master/neural-networks-case-study.md
    // Lets generate a classification dataset that is not easily linearly separable. example is the spiral dataset, which can be generated as follows:
    let mut X: Vec<Vec<f64>> = vec![];
    let rows = data_points * classes;
    // for _ in 0..rows {
    //     X.push(vec![0., 0.]);
    // }
    let mut y = vec![0; rows as usize];
    for class_number in 0..classes {
        let ix = data_points * class_number..data_points * (class_number + 1);
        let radius: Vec<f64> = Array::linspace(0., 1., data_points as usize).to_vec();
        let mut np = vec![];
        let normal = Normal::new(0.0, 1.0).unwrap();
        for _ in 0..data_points {
            np.push(normal.sample(&mut rand::thread_rng()));
        }
        let normal_points: Vec<f64> = np.iter().map(|z| z * 0.2).collect();
        // println!("Normal distribution is {:?}", normal_points);
        let linspace = Array::linspace(
            class_number as f64 * 4.,
            (class_number as f64 + 1.) * 4.,
            data_points as usize,
        );
        // println!("Linspace is {:?}", linspace);
        let mut theta = vec![];
        for (n, i) in linspace.iter().enumerate() {
            theta.push(normal_points[n] + i);
        }
        // println!("Theta is {:?}", theta);
        let mut sin_value: Vec<f64> =
            element_wise_multiplication(&radius, &theta.iter().map(|x| (x * 2.5).sin()).collect());
        let mut cos_value: Vec<f64> =
            element_wise_multiplication(&radius, &theta.iter().map(|x| (x * 2.5).cos()).collect());
        // println!("{:?}", sin_value);
        let mut sin_cos: Vec<Vec<f64>> = vec![];
        for (n, _) in sin_value.iter().enumerate() {
            sin_cos.push(vec![sin_value[n].clone(), cos_value[n].clone()]);
        }
        println!("\n\n=> Ran till here\n");
        for i in 0..sin_cos.len() {
            X.push(sin_cos[i as usize].clone());
        }
        for i in ix {
            y[i as usize] = class_number;
        }
    }
    (X, y)
}

pub fn element_wise_multiplication(a: &Vec<f64>, b: &Vec<f64>) -> Vec<f64> {
    a.iter().zip(b.iter()).map(|(x, y)| *x * *y).collect()
}

/*
OUTPUT
The output from relu for [0.0, 2.0, -1.0, 3.3, -2.7, 1.1, 2.2, -100.0] is [0.0, 2.0, 0.0, 3.3, 0.0, 1.1, 2.2, 0.0]
The input of layer 1 are = 300x2
The matrix is:
[-0.0, 0.0]
[-0.00516091, 0.00868305]
[0.00728493, 0.01884281]
[0.01681935, 0.02520681]
[0.03132465, 0.02551965]
[0.04953345, 0.00985888]
[-0.01125453, 0.05955191]
[0.06971777, 0.01178653]
[0.07475761, 0.03067972]
[0.07368058, 0.05325068]
[0.08355785, 0.05675497]
[0.10513385, -0.03595209]
[0.1201585, 0.01594718]
[0.1085376, 0.07391027]
[0.12504202, -0.06604887]
[0.15077322, 0.01497593]
[0.1612846, -0.01034702]
[0.16507947, -0.04728167]
[0.15479067, -0.09538187]
[0.08224893, -0.17340153]
[0.15544728, -0.12902831]
[0.18026233, -0.11180743]
[0.06351331, -0.21295252]
[0.1836439, -0.14229899]
[0.20333132, -0.13200715]
[0.21943362, -0.12497156]
[0.22270661, -0.13919167]
[0.27023173, -0.03681]
[-0.14273458, -0.24416936]
[0.15684311, -0.24740212]
[0.12581543, -0.27567706]
[-0.05828905, -0.30765826]
[-0.19929391, -0.25448197]
[-0.16527395, -0.28947475]
[-0.24876785, -0.23677353]
[0.17294213, -0.30834764]
[-0.12363592, -0.34197305]
[-0.08893922, -0.36300061]
[-0.38358177, 0.01403328]
[-0.28471337, -0.2722619]
[-0.21616786, -0.34135041]
[-0.40732883, -0.07480863]
[-0.31667974, -0.28230405]
[-0.41555336, -0.12637097]
[-0.37281184, -0.24195495]
[-0.45362893, -0.02885067]
[-0.45917725, -0.07108159]
[-0.29446736, 0.37238977]
[-0.4690081, 0.12292052]
[-0.46554511, -0.16805579]
[-0.50346718, -0.03996013]
[-0.49366671, 0.14722181]
[-0.52031011, 0.07188609]
[0.03447556, 0.53424231]
[-0.38733612, 0.38404608]
[-0.54919937, 0.08379755]
[-0.09528752, 0.55757299]
[-0.4167188, 0.39729363]
[-0.41074984, 0.41774974]
[-0.55915378, 0.2061914]
[-0.01920884, 0.60575612]
[0.30432575, 0.53576205]
[0.50438742, 0.37121181]
[0.29628457, 0.56318215]
[-0.01591099, 0.64626881]
[-0.06671935, 0.65316689]
[0.26252297, 0.61280187]
[0.58138702, 0.34641538]
[0.63077656, -0.27186307]
[0.52513759, 0.45825459]
[0.54602002, 0.44923393]
[0.67455124, 0.24354854]
[0.7262127, -0.03925221]
[0.71365323, 0.18552385]
[0.74334603, 0.07845495]
[0.66016322, 0.37162555]
[0.69569832, -0.32454809]
[0.48454233, 0.60840529]
[0.78226623, -0.09387506]
[0.75169523, -0.26781718]
[0.80642069, -0.05177122]
[0.74945454, -0.32823678]
[-0.12784444, -0.81835704]
[0.70440734, -0.45464026]
[0.77973684, -0.33456987]
[0.76821386, 0.38342813]
[0.11382358, -0.86119746]
[0.25206503, -0.84186184]
[0.39688881, -0.79536327]
[0.30692816, -0.84497215]
[-0.2062092, -0.88539485]
[-0.0644788, -0.91692762]
[-0.64504364, -0.66895744]
[0.00787441, -0.93936094]
[0.13923375, -0.93923087]
[0.23816483, -0.92957083]
[-0.83688372, -0.48983451]
[-0.15289749, -0.96779463]
[-0.47895471, -0.86631541]
[-0.72878365, -0.68474403]
[-0.0, -0.0]
[-0.00919274, -0.00418616]
[-0.01633593, -0.01188524]
[-0.02534119, -0.01661619]
[-0.03041309, 0.02659945]
[-0.04598748, -0.02087851]
[-0.02958055, -0.05289693]
[-0.05355656, -0.04616476]
[-0.07993285, 0.01186111]
[-0.09088087, -0.00226505]
[-0.09223926, 0.04116989]
[-0.11080025, -0.00830558]
[-0.1127479, 0.04450043]
[-0.10715634, 0.07589899]
[-0.10649079, 0.09304661]
[-0.14983936, 0.02247237]
[-0.13281305, 0.09208951]
[-0.1342082, 0.10712118]
[-0.14579523, 0.10863519]
[-0.12775384, 0.14321987]
[-0.09158649, 0.18006687]
[-0.14973004, 0.15025419]
[-0.17015886, 0.14292893]
[0.03255142, 0.2300315]
[-0.12535223, 0.2075002]
[-0.00767758, 0.25240851]
[0.130496, 0.22791083]
[0.12031219, 0.24475527]
[0.00948547, 0.28266918]
[-0.02233174, 0.29207681]
[0.30291562, 0.00833623]
[0.14438501, 0.27785642]
[0.03248456, 0.32159585]
[0.26042117, 0.20806712]
[0.30990135, 0.14801454]
[0.25781381, 0.2419076]
[0.34844646, 0.10400227]
[0.37331346, -0.01779565]
[0.23395375, 0.30429845]
[0.37594946, 0.11768708]
[0.31806297, 0.24916781]
[0.39439859, 0.12634423]
[0.38861274, 0.17018159]
[0.42079013, -0.10765633]
[0.33814225, -0.28842795]
[0.45352681, 0.03041384]
[0.46426058, 0.01893268]
[0.46410435, -0.09996156]
[0.47630753, 0.09060457]
[0.45383887, -0.19749755]
[0.41125857, -0.29315934]
[0.20017196, -0.4746707]
[0.10612365, -0.51442005]
[0.087665, -0.52812712]
[0.07501662, -0.54027138]
[0.42054166, -0.36302436]
[0.07330837, -0.56088611]
[-0.4857719, -0.30906705]
[0.04593886, -0.58405471]
[-0.54504747, -0.24102095]
[-0.24453823, -0.5545363]
[-0.33337784, -0.51818371]
[-0.49548707, -0.38301102]
[-0.45696591, -0.4428779]
[-0.59786979, -0.24590292]
[-0.60285193, -0.26009231]
[-0.50571048, -0.4343977]
[-0.56965882, -0.36538106]
[-0.65314153, -0.21259053]
[-0.5187661, -0.46545514]
[-0.69447298, 0.13287687]
[-0.68838867, -0.20113755]
[-0.71990391, 0.10326657]
[-0.73699171, -0.02373297]
[-0.47310833, 0.57869423]
[-0.58860565, 0.4769323]
[-0.76172684, 0.09539307]
[-0.74904444, -0.20945334]
[-0.7838861, 0.07921847]
[-0.36913163, 0.70746986]
[-0.51357175, 0.62388994]
[-0.00562481, 0.81816248]
[0.04996462, 0.82677444]
[0.33004282, 0.77068748]
[-0.35614902, 0.77011974]
[-0.61544591, 0.59866185]
[0.36328358, 0.78907662]
[0.13956623, 0.86763437]
[-0.34721639, 0.81826905]
[0.67840795, 0.58986904]
[0.45383771, 0.78770401]
[0.01578741, 0.91905633]
[0.56143525, 0.74052401]
[0.63484944, 0.69240679]
[0.46252081, 0.82922564]
[0.84683196, 0.4513314]
[-0.49537071, 0.83361866]
[0.85411195, 0.48010089]
[0.90722356, -0.39603716]
[0.86309705, 0.50503809]
[0.0, -0.0]
[0.00922752, 0.00410892]
[0.01645598, 0.01171846]
[0.0231111, 0.01959976]
[0.03490314, -0.02035331]
[0.02831638, 0.04182036]
[0.05192212, 0.03126001]
[0.06045781, -0.03666529]
[0.07971052, 0.01327322]
[0.06569038, -0.06284296]
[-0.00371018, -0.10094194]
[0.11092219, -0.00647656]
[0.12114933, -0.00390097]
[0.04463725, -0.12349354]
[0.06735709, -0.1243422]
[0.0528691, -0.1419919]
[-0.00600427, -0.16150459]
[0.03318843, -0.16847942]
[-0.12869625, -0.12843336]
[0.12255906, -0.14768972]
[0.05404392, -0.19465718]
[-0.1177781, -0.17641918]
[-0.21877417, -0.0389946]
[-0.09663425, -0.21127211]
[-0.14326284, -0.19556399]
[-0.08447668, -0.23797624]
[-0.17153533, -0.19886726]
[-0.093675, -0.25613504]
[-0.12778305, -0.25231593]
[-0.29080132, -0.03524433]
[-0.23068156, -0.19650288]
[-0.29478504, 0.10560775]
[-0.30168081, -0.11605096]
[-0.23167059, -0.23966612]
[-0.26215623, -0.22185865]
[-0.35237645, 0.02860215]
[-0.32389886, 0.16529045]
[-0.32080747, 0.1917347]
[-0.26767714, 0.27510154]
[-0.39387547, 0.00709626]
[-0.40363787, -0.01803096]
[-0.32473301, 0.25702447]
[0.03790391, 0.42254577]
[-0.00695952, 0.43428767]
[-0.38627038, 0.21983188]
[-0.03914569, 0.45285669]
[-0.27401478, 0.37524956]
[0.14070654, 0.45341684]
[-0.35481348, 0.33043222]
[-0.19176137, 0.45629221]
[0.25438698, 0.4363064]
[0.36532562, 0.36320555]
[-0.21159403, 0.48074752]
[0.20276423, 0.49546955]
[0.22734986, 0.49581519]
[0.19267147, 0.5210755]
[0.06161658, 0.56229063]
[0.52486292, 0.23667636]
[0.35655418, 0.46486493]
[0.51908575, 0.29277606]
[0.56527142, 0.21858105]
[0.2763554, 0.5507112]
[0.62248845, 0.06865134]
[0.5422938, 0.33298064]
[0.37626268, 0.52568331]
[0.45381724, -0.47447695]
[0.65221601, 0.13805331]
[0.6566413, 0.16381908]
[0.49538035, -0.47580133]
[0.55523354, -0.42128669]
[0.69779326, -0.11416458]
[0.28914718, -0.65629961]
[0.71695902, -0.12204663]
[0.70357928, -0.2206722]
[0.34530931, -0.66293286]
[0.17056641, -0.73812474]
[0.56126234, -0.52374823]
[0.37169583, -0.68321335]
[0.16243502, -0.77095256]
[0.36114304, -0.71158096]
[-0.66644842, -0.45699135]
[0.34674865, -0.74107143]
[-0.10999504, -0.82094673]
[-0.11606417, -0.83031113]
[-0.12305472, -0.83951419]
[0.37216708, -0.77373209]
[-0.67625132, -0.54525317]
[-0.41026373, -0.77714336]
[-0.67154434, -0.58236729]
[-0.30623357, -0.84522414]
[-0.2734509, -0.86698955]
[-0.83133142, -0.39217579]
[-0.85507925, -0.36390222]
[-0.83270549, -0.43481323]
[-0.62415482, -0.71552178]
[-0.82314864, -0.49320455]
[-0.90487331, -0.34859218]
[-0.96736649, -0.15558329]
[-0.85245484, 0.50321045]
[-0.76155115, 0.64810482]


The weights of layer 1 are = 2x5
Generated weights are:
[0.095, 0.417, 0.366, -0.717, 0.319]
[-0.632, -0.708, 0.912, -0.737, -0.681]


The bias of layer 1 is  = 1x5
[0.0, 0.0, 0.0, 0.0, 0.0]

Before activation it was [0.0, -0.384, -0.122, -0.443, -0.026]
After activation it was [0.0, 0.0, 0.0, 0.0, 0.0]
The output of layer 1 are = 300x5
Output before activation is:
[0.0, -0.384, -0.122, -0.443, -0.026]
[0.0, -0.436, -0.165, -0.655, -0.155]
[0.0, 0.546, 0.131, 0.382, -0.164]
[0.0, -0.432, -0.067, -0.091, 0.303]
[0.0, -0.418, -0.151, -0.588, -0.116]
[-0.005, -0.309, -0.109, -0.517, -0.193]
[-0.008, -0.252, -0.168, -0.581, -0.317]
[0.007, 0.6, 0.083, 0.745, 0.116]
[-0.002, -0.613, -0.003, -0.598, 0.044]
[-0.007, -0.267, -0.15, -0.558, -0.278]
[-0.011, -0.186, -0.106, -0.517, -0.263]
[-0.01, -0.052, -0.172, -0.564, -0.283]
[0.02, 0.524, 0.069, 0.773, 0.4]
[-0.019, -0.635, 0.017, -0.645, -0.338]
[-0.01, -0.091, -0.151, -0.547, -0.275]
[-0.014, -0.327, -0.142, -0.455, -0.275]
[-0.01, -0.275, -0.149, -0.408, -0.31]
[0.03, 0.623, 0.222, 0.824, 0.394]
[-0.03, -0.627, -0.192, -0.804, -0.315]
[-0.011, -0.289, -0.146, -0.419, -0.297]
[-0.013, -0.409, -0.143, -0.52, -0.175]
[-0.005, -0.464, -0.199, -0.693, -0.316]
[0.035, 0.584, 0.144, 0.572, 0.06]
[-0.041, -0.464, -0.063, -0.312, 0.115]
[-0.007, -0.445, -0.181, -0.638, -0.272]
[-0.001, -0.419, -0.16, -0.436, -0.289]
[0.014, -0.49, -0.181, -0.68, -0.336]
[0.028, 0.572, 0.228, 0.321, 0.399]
[-0.042, -0.433, -0.18, 0.001, -0.305]
[0.01, -0.466, -0.174, -0.604, -0.32]
[-0.038, -0.362, -0.131, -0.464, -0.263]
[-0.046, -0.324, -0.106, -0.407, -0.379]
[0.051, 0.655, 0.256, 0.853, 0.242]
[-0.035, -0.639, -0.261, -0.842, -0.08]
[-0.044, -0.333, -0.113, -0.421, -0.342]
[0.0, -0.163, -0.143, -0.535, -0.273]
[0.021, -0.002, -0.123, -0.556, -0.262]
[0.037, 0.529, 0.268, 0.843, 0.466]
[-0.058, -0.672, -0.266, -0.739, -0.435]
[0.015, -0.05, -0.128, -0.546, -0.263]
[-0.012, 0.232, -0.177, -0.55, -0.242]
[0.01, 0.456, -0.196, -0.724, -0.381]
[0.056, -0.017, 0.262, 0.62, 0.172]
[-0.076, -0.251, -0.215, -0.354, 0.011]
[0.003, 0.387, -0.189, -0.668, -0.338]
[-0.026, -0.239, -0.186, -0.308, -0.306]
[-0.006, -0.105, -0.216, -0.134, -0.403]
[0.076, 0.611, 0.259, 0.787, 0.346]
[-0.092, -0.714, -0.199, -0.921, -0.198]
[-0.012, -0.144, -0.206, -0.185, -0.371]
[-0.027, -0.232, 0.024, -0.454, -0.251]
[-0.005, -0.09, 0.121, -0.368, -0.202]
[0.083, 0.61, 0.119, 0.885, 0.492]
[-0.101, -0.722, -0.223, -0.905, -0.503]
[-0.011, -0.131, 0.091, -0.391, -0.215]
[0.033, -0.089, -0.161, -0.579, -0.194]
[0.07, 0.109, -0.136, -0.644, -0.104]
[0.006, 0.47, 0.307, 0.844, 0.465]
[-0.048, -0.663, -0.308, -0.688, -0.529]
[0.059, 0.05, -0.143, -0.62, -0.13]
[0.002, 0.094, -0.2, -0.414, -0.323]
[0.039, 0.331, -0.214, -0.29, -0.428]
[0.059, 0.23, 0.306, 0.881, 0.361]
[-0.097, -0.491, -0.26, -0.948, -0.202]
[0.028, 0.259, -0.208, -0.325, -0.394]
[-0.036, -0.049, -0.106, -0.377, -0.293]
[-0.007, 0.167, -0.038, -0.225, -0.266]
[0.108, 0.431, 0.286, 0.864, 0.527]
[-0.132, -0.648, -0.34, -0.965, -0.51]
[-0.015, 0.102, -0.058, -0.269, -0.272]
[0.054, 0.022, -0.064, -0.48, -0.291]
[0.099, 0.255, 0.025, -0.394, -0.256]
[-0.014, 0.344, 0.249, 0.926, 0.536]
[-0.04, -0.59, -0.331, -0.942, -0.528]
[0.085, 0.184, -0.001, -0.417, -0.265]
[0.005, -0.172, -0.128, -0.204, -0.311]
[0.053, 0.013, -0.063, 0.034, -0.288]
[0.069, 0.581, 0.315, 0.722, 0.546]
[-0.119, -0.747, -0.363, -0.939, -0.522]
[0.038, -0.042, -0.082, -0.037, -0.293]
[0.022, 0.272, -0.032, -0.573, -0.349]
[0.075, 0.52, 0.072, -0.796, -0.372]
[0.05, -0.041, 0.223, 0.579, 0.536]
[-0.108, -0.259, -0.326, -0.259, -0.458]
[0.059, 0.443, 0.041, -0.725, -0.363]
[0.046, -0.338, 0.047, -0.222, -0.099]
[0.103, -0.228, 0.169, 0.017, 0.052]
[0.018, 0.733, 0.121, 0.751, 0.408]
[-0.083, -0.795, -0.254, -0.966, -0.55]
[0.085, -0.259, 0.132, -0.054, 0.007]
[0.075, 0.134, -0.17, 0.337, -0.259]
[0.133, 0.393, -0.117, 0.659, -0.18]
[-0.03, 0.201, 0.364, -0.029, 0.555]
[-0.04, -0.491, -0.392, -0.358, -0.598]
[0.115, 0.314, -0.132, 0.56, -0.202]
[0.118, 0.241, -0.038, -0.237, -0.135]
[0.158, 0.504, 0.074, 0.003, 0.01]
[-0.128, 0.031, 0.245, 0.777, 0.457]
[0.069, -0.341, -0.356, -0.991, -0.587]
[0.145, 0.423, 0.04, -0.068, -0.033]
[0.097, 0.11, -0.127, 0.0, -0.084]
[0.157, 0.373, -0.043, 0.0, 0.081]
[-0.06, 0.248, 0.344, 0.0, 0.407]
[-0.016, -0.54, -0.411, 0.0, -0.566]
[0.138, 0.293, -0.068, 0.0, 0.032]
[0.088, 0.279, -0.042, -0.001, -0.321]
[0.155, 0.545, 0.076, 0.001, -0.274]
[-0.035, -0.025, 0.26, 0.008, 0.604]
[-0.046, -0.295, -0.375, -0.009, -0.604]
[0.134, 0.463, 0.04, 0.001, -0.286]
[0.141, 0.506, -0.07, -0.005, 0.016]
[0.178, 0.527, 0.042, -0.001, 0.211]
[-0.17, -0.793, 0.298, 0.017, 0.291]
[0.112, 0.695, -0.404, -0.02, -0.496]
[0.166, 0.517, 0.009, -0.002, 0.152]
[0.108, 0.355, 0.109, -0.01, -0.158]
[0.178, 0.616, 0.252, -0.004, -0.009]
[-0.062, -0.156, 0.056, 0.027, 0.503]
[-0.026, -0.169, -0.222, -0.031, -0.634]
[0.156, 0.535, 0.208, -0.005, -0.053]
[0.103, 0.286, 0.215, 0.017, -0.296]
[0.179, 0.563, 0.346, 0.029, -0.215]
[-0.045, -0.019, -0.139, -0.005, 0.618]
[-0.048, -0.312, -0.029, -0.01, -0.657]
[0.155, 0.477, 0.305, 0.025, -0.237]
[0.1, -0.169, 0.024, -0.023, 0.343]
[0.18, 0.049, 0.168, -0.017, 0.526]
[-0.033, 0.631, 0.194, 0.049, -0.266]
[-0.065, -0.833, -0.347, -0.051, 0.025]
[0.156, -0.016, 0.124, -0.019, 0.468]
[0.11, 0.556, 0.033, -0.014, -0.025]
[0.192, 0.658, 0.181, 0.0, 0.175]
[-0.045, -0.743, 0.188, 0.048, 0.365]
[-0.057, 0.554, -0.346, -0.06, -0.569]
[0.166, 0.623, 0.136, -0.004, 0.115]
[0.049, 0.557, 0.108, 0.029, -0.041]
[0.139, 0.702, 0.265, 0.052, 0.158]
[0.066, -0.675, 0.079, -0.011, 0.39]
[-0.166, 0.44, -0.259, -0.016, -0.591]
[0.112, 0.654, 0.217, 0.045, 0.098]
[0.141, 0.541, -0.012, 0.0, 0.348]
[0.114, 0.729, 0.135, 0.024, 0.544]
[-0.274, -0.58, 0.257, 0.042, -0.252]
[0.283, 0.302, -0.408, -0.066, -0.004]
[0.121, 0.669, 0.091, 0.017, 0.483]
[0.172, 0.564, 0.168, 0.046, 0.32]
[0.241, 0.727, 0.33, 0.072, 0.53]
[-0.168, -0.658, -0.014, -0.033, -0.18]
[0.07, 0.403, -0.179, 0.0, -0.087]
[0.219, 0.674, 0.28, 0.064, 0.465]
[0.187, 0.54, 0.225, 0.064, 0.139]
[0.248, 0.541, 0.38, 0.07, 0.372]
[-0.205, -0.882, -0.116, -0.093, 0.152]
[0.113, 0.801, -0.078, 0.078, -0.416]
[0.228, 0.538, 0.331, 0.068, 0.301]
[0.189, 0.574, 0.32, 0.015, 0.443]
[0.194, 0.623, 0.42, 0.051, 0.586]
[-0.301, -0.859, -0.359, 0.035, -0.492]
[0.269, 0.723, 0.207, -0.074, 0.277]
[0.191, 0.604, 0.388, 0.04, 0.54]
[0.142, 0.362, 0.336, 0.014, 0.146]
[0.098, 0.205, 0.409, 0.054, 0.386]
[-0.305, -0.846, -0.43, 0.041, 0.152]
[0.331, 0.956, 0.304, -0.083, -0.424]
[0.11, 0.25, 0.385, 0.042, 0.312]
[0.168, 0.595, 0.343, 0.083, 0.207]
[0.137, 0.669, 0.411, 0.107, 0.45]
[-0.324, -0.853, -0.449, -0.096, 0.057]
[0.332, 0.687, 0.327, 0.06, -0.341]
[0.145, 0.643, 0.388, 0.099, 0.375]
[0.127, 0.607, 0.349, 0.085, 0.452]
[0.064, 0.724, 0.414, 0.117, 0.614]
[-0.306, -0.805, -0.465, -0.088, -0.478]
[0.353, 0.593, 0.345, 0.044, 0.241]
[0.082, 0.685, 0.392, 0.107, 0.562]
[0.212, 0.611, 0.27, 0.095, 0.483]
[0.291, 0.758, 0.433, 0.123, 0.594]
[-0.217, -0.76, -0.177, -0.11, -0.61]
[0.104, 0.515, -0.033, 0.067, 0.422]
[0.266, 0.71, 0.382, 0.114, 0.558]
[0.205, 0.231, 0.362, 0.102, 0.385]
[0.191, -0.002, 0.428, 0.112, 0.605]
[-0.357, -0.753, -0.484, -0.149, -0.272]
[0.341, 0.962, 0.361, 0.124, -0.016]
[0.194, 0.067, 0.406, 0.109, 0.536]
[0.221, 0.598, 0.15, 0.11, 0.468]
[0.22, 0.622, 0.017, 0.134, 0.639]
[-0.363, -0.938, -0.459, -0.141, -0.487]
[0.332, 0.823, 0.577, 0.101, 0.238]
[0.219, 0.611, 0.056, 0.126, 0.584]
[-0.045, 0.503, 0.374, 0.069, 0.503]
[-0.169, 0.414, 0.433, 0.038, 0.614]
[-0.127, -0.965, -0.515, -0.164, -0.643]
[0.265, 0.982, 0.398, 0.187, 0.452]
[-0.131, 0.438, 0.413, 0.047, 0.577]
[0.146, 0.364, 0.101, 0.105, 0.485]
[0.075, 0.181, -0.056, 0.156, 0.655]
[-0.352, -0.891, -0.419, -0.089, -0.516]
[0.405, 1.028, 0.569, 0.021, 0.266]
[0.095, 0.234, -0.009, 0.14, 0.6]
[0.196, 0.0, 0.328, 0.129, 0.226]
[0.152, 0.0, 0.291, 0.161, 0.046]
[-0.39, 0.0, -0.595, -0.157, -0.66]
[0.407, 0.0, 0.585, 0.105, 0.815]
[0.164, 0.0, 0.3, 0.15, 0.099]
[0.009, 0.002, 0.296, 0.101, 0.502]
[-0.116, 0.0, 0.228, 0.076, 0.67]
[-0.217, -0.007, -0.594, -0.204, -0.548]
[0.348, 0.01, 0.621, 0.215, 0.298]
[-0.078, 0.0, 0.247, 0.083, 0.616]
[0.149, 0.006, 0.195, 0.004, 0.509]
[0.068, 0.002, 0.065, -0.063, 0.536]
[-0.373, -0.016, -0.53, -0.115, -0.788]
[0.436, 0.021, 0.638, 0.186, 0.684]
[0.092, 0.003, 0.103, -0.043, 0.524]
[0.041, 0.009, 0.237, 0.125, 0.514]
[-0.083, 0.002, 0.124, 0.11, 0.54]
[-0.267, -0.024, -0.571, -0.228, -0.799]
[0.392, 0.031, 0.655, 0.225, 0.696]
[-0.046, 0.004, 0.156, 0.114, 0.529]
[0.118, -0.019, 0.099, 0.11, 0.519]
[0.016, -0.031, -0.075, 0.079, 0.544]
[-0.357, 0.014, -0.443, -0.23, -0.81]
[0.446, 0.003, 0.61, 0.247, 0.707]
[0.046, -0.027, -0.023, 0.088, 0.533]
[-0.024, 0.009, 0.108, 0.143, 0.525]
[-0.168, -0.004, -0.067, 0.134, 0.703]
[-0.192, -0.035, -0.457, -0.247, -0.569]
[0.347, 0.049, 0.624, 0.236, 0.304]
[-0.125, 0.0, -0.015, 0.136, 0.646]
[0.002, 0.031, 0.227, 0.11, 0.281]
[-0.141, 0.026, 0.097, 0.07, 0.105]
[-0.232, -0.059, -0.581, -0.244, -0.744]
[0.382, 0.061, 0.683, 0.27, 0.887]
[-0.098, 0.027, 0.135, 0.081, 0.156]
[-0.263, 0.025, 0.177, 0.153, 0.453]
[-0.386, 0.011, 0.022, 0.143, 0.38]
[0.232, -0.061, -0.541, -0.267, -0.858]
[-0.063, 0.073, 0.678, 0.256, 0.867]
[-0.347, 0.015, 0.068, 0.145, 0.399]
[-0.122, -0.015, 0.073, 0.148, 0.305]
[-0.282, -0.041, -0.121, 0.126, 0.133]
[-0.059, -0.018, -0.432, -0.276, -0.776]
[0.246, 0.049, 0.625, 0.278, 0.911]
[-0.233, -0.033, -0.063, 0.132, 0.183]
[0.062, -0.007, 0.245, -0.005, 0.506]
[-0.075, -0.036, 0.114, -0.096, 0.471]
[-0.323, -0.035, -0.614, -0.138, -0.882]
[0.458, 0.067, 0.715, 0.235, 0.843]
[-0.034, -0.027, 0.152, -0.068, 0.478]
[-0.022, -0.034, -0.149, 0.103, 0.522]
[-0.181, -0.067, -0.383, 0.043, 0.5]
[-0.22, 0.004, -0.132, -0.263, -0.89]
[0.391, 0.036, 0.401, 0.311, 0.836]
[-0.133, -0.057, -0.312, 0.061, 0.504]
[-0.139, -0.005, 0.062, -0.094, 0.169]
[-0.31, -0.04, -0.144, -0.197, -0.069]
[-0.046, -0.048, -0.435, -0.011, -0.661]
[0.246, 0.086, 0.642, 0.134, 0.886]
[-0.257, -0.029, -0.082, -0.165, 0.002]
[-0.094, -0.038, -0.133, 0.045, 0.149]
[-0.267, -0.078, -0.373, -0.043, -0.098]
[-0.124, 0.0, -0.169, -0.216, -0.644]
[0.321, 0.049, 0.441, 0.302, 0.882]
[-0.214, -0.066, -0.299, -0.017, -0.024]
[-0.334, -0.058, -0.055, 0.13, 0.196]
[-0.363, -0.098, -0.29, 0.074, -0.039]
[0.5, 0.031, -0.291, -0.303, -0.701]
[-0.418, 0.021, 0.546, 0.343, 0.918]
[-0.352, -0.085, -0.218, 0.09, 0.031]
[-0.279, -0.068, -0.41, 0.116, 0.393]
[-0.433, -0.11, -0.607, 0.048, 0.247]
[0.209, 0.046, 0.355, -0.298, -0.88]
[-0.005, 0.008, -0.087, 0.352, 0.975]
[-0.385, -0.097, -0.545, 0.068, 0.289]
[-0.105, -0.028, -0.357, -0.051, 0.234]
[-0.288, -0.078, -0.583, -0.167, 0.006]
[-0.124, -0.034, 0.22, -0.102, -0.751]
[0.333, 0.091, 0.071, 0.232, 0.954]
[-0.232, -0.063, -0.512, -0.131, 0.074]
[-0.361, -0.07, -0.132, -0.135, 0.135]
[-0.434, -0.12, -0.385, -0.252, -0.13]
[0.474, 0.036, -0.191, 0.033, -0.649]
[-0.342, 0.028, 0.476, 0.111, 0.906]
[-0.41, -0.105, -0.307, -0.215, -0.051]
[-0.29, -0.08, 0.062, -0.151, 0.007]
[-0.455, -0.131, -0.164, -0.269, -0.293]
[0.21, 0.049, -0.465, 0.058, -0.495]
[0.006, 0.018, 0.692, 0.089, 0.809]
[-0.403, -0.115, -0.096, -0.232, -0.202]
[-0.303, -0.082, -0.124, -0.199, -0.399]
[-0.467, -0.137, -0.382, -0.306, -0.711]
[0.231, 0.046, -0.214, 0.153, 0.147]
[-0.013, 0.025, 0.504, -0.01, 0.241]
[-0.415, -0.12, -0.304, -0.272, -0.614]
[-0.183, -0.102, -0.482, -0.041, -0.481]
[-0.379, -0.154, -0.654, -0.169, -0.776]
[-0.016, 0.084, 0.511, -0.137, 0.313]
[0.249, -0.013, -0.256, 0.278, 0.069]
[-0.318, -0.138, -0.599, -0.13, -0.684]


Input to second layer is:
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.546, 0.131, 0.382, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.303]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.007, 0.6, 0.083, 0.745, 0.116]
[0.0, 0.0, 0.0, 0.0, 0.044]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.02, 0.524, 0.069, 0.773, 0.4]
[0.0, 0.0, 0.017, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.03, 0.623, 0.222, 0.824, 0.394]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.035, 0.584, 0.144, 0.572, 0.06]
[0.0, 0.0, 0.0, 0.0, 0.115]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.014, 0.0, 0.0, 0.0, 0.0]
[0.028, 0.572, 0.228, 0.321, 0.399]
[0.0, 0.0, 0.0, 0.001, 0.0]
[0.01, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.051, 0.655, 0.256, 0.853, 0.242]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.021, 0.0, 0.0, 0.0, 0.0]
[0.037, 0.529, 0.268, 0.843, 0.466]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.015, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.232, 0.0, 0.0, 0.0]
[0.01, 0.456, 0.0, 0.0, 0.0]
[0.056, 0.0, 0.262, 0.62, 0.172]
[0.0, 0.0, 0.0, 0.0, 0.011]
[0.003, 0.387, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.076, 0.611, 0.259, 0.787, 0.346]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.024, 0.0, 0.0]
[0.0, 0.0, 0.121, 0.0, 0.0]
[0.083, 0.61, 0.119, 0.885, 0.492]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.091, 0.0, 0.0]
[0.033, 0.0, 0.0, 0.0, 0.0]
[0.07, 0.109, 0.0, 0.0, 0.0]
[0.006, 0.47, 0.307, 0.844, 0.465]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.059, 0.05, 0.0, 0.0, 0.0]
[0.002, 0.094, 0.0, 0.0, 0.0]
[0.039, 0.331, 0.0, 0.0, 0.0]
[0.059, 0.23, 0.306, 0.881, 0.361]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.028, 0.259, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.167, 0.0, 0.0, 0.0]
[0.108, 0.431, 0.286, 0.864, 0.527]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.102, 0.0, 0.0, 0.0]
[0.054, 0.022, 0.0, 0.0, 0.0]
[0.099, 0.255, 0.025, 0.0, 0.0]
[0.0, 0.344, 0.249, 0.926, 0.536]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.085, 0.184, 0.0, 0.0, 0.0]
[0.005, 0.0, 0.0, 0.0, 0.0]
[0.053, 0.013, 0.0, 0.034, 0.0]
[0.069, 0.581, 0.315, 0.722, 0.546]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.038, 0.0, 0.0, 0.0, 0.0]
[0.022, 0.272, 0.0, 0.0, 0.0]
[0.075, 0.52, 0.072, 0.0, 0.0]
[0.05, 0.0, 0.223, 0.579, 0.536]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.059, 0.443, 0.041, 0.0, 0.0]
[0.046, 0.0, 0.047, 0.0, 0.0]
[0.103, 0.0, 0.169, 0.017, 0.052]
[0.018, 0.733, 0.121, 0.751, 0.408]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.085, 0.0, 0.132, 0.0, 0.007]
[0.075, 0.134, 0.0, 0.337, 0.0]
[0.133, 0.393, 0.0, 0.659, 0.0]
[0.0, 0.201, 0.364, 0.0, 0.555]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.115, 0.314, 0.0, 0.56, 0.0]
[0.118, 0.241, 0.0, 0.0, 0.0]
[0.158, 0.504, 0.074, 0.003, 0.01]
[0.0, 0.031, 0.245, 0.777, 0.457]
[0.069, 0.0, 0.0, 0.0, 0.0]
[0.145, 0.423, 0.04, 0.0, 0.0]
[0.097, 0.11, 0.0, 0.0, 0.0]
[0.157, 0.373, 0.0, 0.0, 0.081]
[0.0, 0.248, 0.344, 0.0, 0.407]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.138, 0.293, 0.0, 0.0, 0.032]
[0.088, 0.279, 0.0, 0.0, 0.0]
[0.155, 0.545, 0.076, 0.001, 0.0]
[0.0, 0.0, 0.26, 0.008, 0.604]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.134, 0.463, 0.04, 0.001, 0.0]
[0.141, 0.506, 0.0, 0.0, 0.016]
[0.178, 0.527, 0.042, 0.0, 0.211]
[0.0, 0.0, 0.298, 0.017, 0.291]
[0.112, 0.695, 0.0, 0.0, 0.0]
[0.166, 0.517, 0.009, 0.0, 0.152]
[0.108, 0.355, 0.109, 0.0, 0.0]
[0.178, 0.616, 0.252, 0.0, 0.0]
[0.0, 0.0, 0.056, 0.027, 0.503]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.156, 0.535, 0.208, 0.0, 0.0]
[0.103, 0.286, 0.215, 0.017, 0.0]
[0.179, 0.563, 0.346, 0.029, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.618]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.155, 0.477, 0.305, 0.025, 0.0]
[0.1, 0.0, 0.024, 0.0, 0.343]
[0.18, 0.049, 0.168, 0.0, 0.526]
[0.0, 0.631, 0.194, 0.049, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.025]
[0.156, 0.0, 0.124, 0.0, 0.468]
[0.11, 0.556, 0.033, 0.0, 0.0]
[0.192, 0.658, 0.181, 0.0, 0.175]
[0.0, 0.0, 0.188, 0.048, 0.365]
[0.0, 0.554, 0.0, 0.0, 0.0]
[0.166, 0.623, 0.136, 0.0, 0.115]
[0.049, 0.557, 0.108, 0.029, 0.0]
[0.139, 0.702, 0.265, 0.052, 0.158]
[0.066, 0.0, 0.079, 0.0, 0.39]
[0.0, 0.44, 0.0, 0.0, 0.0]
[0.112, 0.654, 0.217, 0.045, 0.098]
[0.141, 0.541, 0.0, 0.0, 0.348]
[0.114, 0.729, 0.135, 0.024, 0.544]
[0.0, 0.0, 0.257, 0.042, 0.0]
[0.283, 0.302, 0.0, 0.0, 0.0]
[0.121, 0.669, 0.091, 0.017, 0.483]
[0.172, 0.564, 0.168, 0.046, 0.32]
[0.241, 0.727, 0.33, 0.072, 0.53]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.07, 0.403, 0.0, 0.0, 0.0]
[0.219, 0.674, 0.28, 0.064, 0.465]
[0.187, 0.54, 0.225, 0.064, 0.139]
[0.248, 0.541, 0.38, 0.07, 0.372]
[0.0, 0.0, 0.0, 0.0, 0.152]
[0.113, 0.801, 0.0, 0.078, 0.0]
[0.228, 0.538, 0.331, 0.068, 0.301]
[0.189, 0.574, 0.32, 0.015, 0.443]
[0.194, 0.623, 0.42, 0.051, 0.586]
[0.0, 0.0, 0.0, 0.035, 0.0]
[0.269, 0.723, 0.207, 0.0, 0.277]
[0.191, 0.604, 0.388, 0.04, 0.54]
[0.142, 0.362, 0.336, 0.014, 0.146]
[0.098, 0.205, 0.409, 0.054, 0.386]
[0.0, 0.0, 0.0, 0.041, 0.152]
[0.331, 0.956, 0.304, 0.0, 0.0]
[0.11, 0.25, 0.385, 0.042, 0.312]
[0.168, 0.595, 0.343, 0.083, 0.207]
[0.137, 0.669, 0.411, 0.107, 0.45]
[0.0, 0.0, 0.0, 0.0, 0.057]
[0.332, 0.687, 0.327, 0.06, 0.0]
[0.145, 0.643, 0.388, 0.099, 0.375]
[0.127, 0.607, 0.349, 0.085, 0.452]
[0.064, 0.724, 0.414, 0.117, 0.614]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.353, 0.593, 0.345, 0.044, 0.241]
[0.082, 0.685, 0.392, 0.107, 0.562]
[0.212, 0.611, 0.27, 0.095, 0.483]
[0.291, 0.758, 0.433, 0.123, 0.594]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.104, 0.515, 0.0, 0.067, 0.422]
[0.266, 0.71, 0.382, 0.114, 0.558]
[0.205, 0.231, 0.362, 0.102, 0.385]
[0.191, 0.0, 0.428, 0.112, 0.605]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.341, 0.962, 0.361, 0.124, 0.0]
[0.194, 0.067, 0.406, 0.109, 0.536]
[0.221, 0.598, 0.15, 0.11, 0.468]
[0.22, 0.622, 0.017, 0.134, 0.639]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.332, 0.823, 0.577, 0.101, 0.238]
[0.219, 0.611, 0.056, 0.126, 0.584]
[0.0, 0.503, 0.374, 0.069, 0.503]
[0.0, 0.414, 0.433, 0.038, 0.614]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.265, 0.982, 0.398, 0.187, 0.452]
[0.0, 0.438, 0.413, 0.047, 0.577]
[0.146, 0.364, 0.101, 0.105, 0.485]
[0.075, 0.181, 0.0, 0.156, 0.655]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.405, 1.028, 0.569, 0.021, 0.266]
[0.095, 0.234, 0.0, 0.14, 0.6]
[0.196, 0.0, 0.328, 0.129, 0.226]
[0.152, 0.0, 0.291, 0.161, 0.046]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.407, 0.0, 0.585, 0.105, 0.815]
[0.164, 0.0, 0.3, 0.15, 0.099]
[0.009, 0.002, 0.296, 0.101, 0.502]
[0.0, 0.0, 0.228, 0.076, 0.67]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.348, 0.01, 0.621, 0.215, 0.298]
[0.0, 0.0, 0.247, 0.083, 0.616]
[0.149, 0.006, 0.195, 0.004, 0.509]
[0.068, 0.002, 0.065, 0.0, 0.536]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.436, 0.021, 0.638, 0.186, 0.684]
[0.092, 0.003, 0.103, 0.0, 0.524]
[0.041, 0.009, 0.237, 0.125, 0.514]
[0.0, 0.002, 0.124, 0.11, 0.54]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.392, 0.031, 0.655, 0.225, 0.696]
[0.0, 0.004, 0.156, 0.114, 0.529]
[0.118, 0.0, 0.099, 0.11, 0.519]
[0.016, 0.0, 0.0, 0.079, 0.544]
[0.0, 0.014, 0.0, 0.0, 0.0]
[0.446, 0.003, 0.61, 0.247, 0.707]
[0.046, 0.0, 0.0, 0.088, 0.533]
[0.0, 0.009, 0.108, 0.143, 0.525]
[0.0, 0.0, 0.0, 0.134, 0.703]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.347, 0.049, 0.624, 0.236, 0.304]
[0.0, 0.0, 0.0, 0.136, 0.646]
[0.002, 0.031, 0.227, 0.11, 0.281]
[0.0, 0.026, 0.097, 0.07, 0.105]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.382, 0.061, 0.683, 0.27, 0.887]
[0.0, 0.027, 0.135, 0.081, 0.156]
[0.0, 0.025, 0.177, 0.153, 0.453]
[0.0, 0.011, 0.022, 0.143, 0.38]
[0.232, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.073, 0.678, 0.256, 0.867]
[0.0, 0.015, 0.068, 0.145, 0.399]
[0.0, 0.0, 0.073, 0.148, 0.305]
[0.0, 0.0, 0.0, 0.126, 0.133]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.246, 0.049, 0.625, 0.278, 0.911]
[0.0, 0.0, 0.0, 0.132, 0.183]
[0.062, 0.0, 0.245, 0.0, 0.506]
[0.0, 0.0, 0.114, 0.0, 0.471]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.458, 0.067, 0.715, 0.235, 0.843]
[0.0, 0.0, 0.152, 0.0, 0.478]
[0.0, 0.0, 0.0, 0.103, 0.522]
[0.0, 0.0, 0.0, 0.043, 0.5]
[0.0, 0.004, 0.0, 0.0, 0.0]
[0.391, 0.036, 0.401, 0.311, 0.836]
[0.0, 0.0, 0.0, 0.061, 0.504]
[0.0, 0.0, 0.062, 0.0, 0.169]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.246, 0.086, 0.642, 0.134, 0.886]
[0.0, 0.0, 0.0, 0.0, 0.002]
[0.0, 0.0, 0.0, 0.045, 0.149]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.321, 0.049, 0.441, 0.302, 0.882]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.13, 0.196]
[0.0, 0.0, 0.0, 0.074, 0.0]
[0.5, 0.031, 0.0, 0.0, 0.0]
[0.0, 0.021, 0.546, 0.343, 0.918]
[0.0, 0.0, 0.0, 0.09, 0.031]
[0.0, 0.0, 0.0, 0.116, 0.393]
[0.0, 0.0, 0.0, 0.048, 0.247]
[0.209, 0.046, 0.355, 0.0, 0.0]
[0.0, 0.008, 0.0, 0.352, 0.975]
[0.0, 0.0, 0.0, 0.068, 0.289]
[0.0, 0.0, 0.0, 0.0, 0.234]
[0.0, 0.0, 0.0, 0.0, 0.006]
[0.0, 0.0, 0.22, 0.0, 0.0]
[0.333, 0.091, 0.071, 0.232, 0.954]
[0.0, 0.0, 0.0, 0.0, 0.074]
[0.0, 0.0, 0.0, 0.0, 0.135]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.474, 0.036, 0.0, 0.033, 0.0]
[0.0, 0.028, 0.476, 0.111, 0.906]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.062, 0.0, 0.007]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.21, 0.049, 0.0, 0.058, 0.0]
[0.006, 0.018, 0.692, 0.089, 0.809]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.231, 0.046, 0.0, 0.153, 0.147]
[0.0, 0.025, 0.504, 0.0, 0.241]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.084, 0.511, 0.0, 0.313]
[0.249, 0.0, 0.0, 0.278, 0.069]
[0.0, 0.0, 0.0, 0.0, 0.0]
*/
